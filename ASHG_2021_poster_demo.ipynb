{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prioritizing Risk Factors of Heart Failure from UK Biobank Using Explainable Artificial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "Arkarachai Fungtammasan, Chiao-Feng Lin, Daniel Quang, Yih-Chii Hwang, Jason Chin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "Predicting diseases using genetic variation, phenotypic traits, or environmental factors is one of the most important goals for precision health. With the recent advancement in machine learning techniques and the availability of large-scale genotype-phenotype databases, sophisticated machine learning methods have become widely adopted by the scientific community for predicting diseases. Many of these techniques offer more flexibility in terms of model assumption compared to traditional statistical methods and the ability to utilize a large amount of data as they have been developed to handle a larger amount of data in other scientific domains. However, the lack of interpretability and explainability of these models faces challenges in detecting errors and building trust. \n",
    "\n",
    "We demonstrate the use case of explainable artificial intelligence in predicting heart failure, one of the global major causes of death. We extracted the baseline data, blood measured, and reported lifestyle features from UK Biobank using UKB Research Analysis Platform. We experimented with the state-of-art black-box modeling tools such as LightGBM and XGBoost and estimated the contribution of each predictive feature using SHAP. In addition, we also used a recently developed glass-box machine learning model, Explainable Boosting Machine, which directly measures the impact of each predictor and their interactions instead of indirect inference. \n",
    "\n",
    "We found that sophisticated blackbox and glassbox machine learning methods yield similar predictive power. Age and sex are the most important features for most model classes regardless of algorithms, missing value handling strategies, or glassbox vs blackbox feature important analysis. The rest of the important features such as BMI, blood-measured C-reactive protein, cholesterol, Hemoglobin A1c are contributing to the minority of the population, but in large effect. Some features such as BMI also have non-linear contributions to model prediction. Our study exemplifies how machine learning could be used to study the impact of risk factors for individual patient. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "1. Load dataset in to instance with Spark \n",
    "2. Choose fields and put in Spark dataframe\n",
    "3. Use readable field name and save as pandas pickle\n",
    "4. Close Spark and setup data/library in ML_IP instance\n",
    "5. Mark missing values\n",
    "6. Process variables\n",
    "7. Univariate analysis\n",
    "8. Basic machine learning \n",
    "9. Interpretable and explainable ML\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that data exploration has not been included in this demo. Some data exploration could be done using UK Biobank cohort browser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load dataset in to instance with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Spark cluster HAIL-0.2.61 using instance mem1_ssd1_v2_x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dxdata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from scipy import stats\n",
    "import json\n",
    "# Initialize dxdata engine\n",
    "engine = dxdata.connect(dialect=\"hive+pyspark\")\n",
    "pt = engine.execute(\"SET spark.sql.shuffle.partitions=50\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = \"project-G59Y0xQJ51ZZFXpfFpfxQZzj:record-G59f2VjJk7QPJpF04v0619QG\" # app46926_20210928164649.dataset\n",
    "dataset = dxdata.load_dataset(id=DATASET_ID)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \"entities\" are virtual tables linked to one another.\n",
    "\n",
    "The main entity is \"participant\" and corresponds to most pheno fields. Additional entities correspond to linked health care data.\n",
    "Entities starting with \"hesin\" are for hospital records; entities starting with \"gp\" are for GP records, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the main 'participant' entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant = dataset[\"participant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting participant fields by field index, instance index, array index\n",
    "\n",
    "For the main participant pheno entity, RAP uses field names with the following convention:\n",
    "\n",
    "|Type of field|Syntax for field name|Example|\n",
    "|:------------|---------------------|-------|\n",
    "|Neither instanced nor arrayed|`p<FIELD>`|`p31`|\n",
    "|Instanced but not arrayed|`p<FIELD>_i<INSTANCE>`|`p40005_i0`|\n",
    "|Arrayed but not instanced|`p<FIELD>_a<ARRAY>`|`p41262_a0`|\n",
    "|Instanced and arrayed|`p<FIELD>_i<INSTANCE>_a<ARRAY>`|`p93_i0_a0`|\n",
    "\n",
    "If you know exactly the field names you want to work with, put them in a string array (we will see later how to use that):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking up fields by id\n",
    "\n",
    "If you know the field id but you are not sure if it is instanced or arrayed, and want to grab all instances/arrays (if any), use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def field_names_for_id(field_id):\n",
    "    from distutils.version import LooseVersion\n",
    "    field_id = str(field_id)\n",
    "    fields = participant.find_fields(name_regex=r'^p{}(_i\\d+)?(_a\\d+)?$'.format(field_id))\n",
    "    return sorted([field.name for field in fields], key=lambda n: LooseVersion(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking up fields by title\n",
    "\n",
    "If you remember part of the field title, use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fields_by_title_keyword(keyword):\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from distutils.version import LooseVersion\n",
    "    regex = re.compile(\"^.*\" + re.escape(keyword) + \".*$\", re.I)\n",
    "    fields = participant.find_fields(boolean_func=lambda f: getattr(f, \"title\") and bool(regex.match(f.title)))\n",
    "    return pd.DataFrame(data=[(f.name, f.title, \" > \".join(f.folder_path)) for f in sorted(fields, key=lambda n: LooseVersion(n.name))], columns=[\"name\",\"title\", \"category\"])\n",
    "\n",
    "def field_names_by_title_keyword(keyword):\n",
    "    return list(fields_by_title_keyword(keyword)[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Choose fields and put in Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_code_mapping={'BMI': 21001,\n",
    " 'HDL': 30760,\n",
    " 'LDL': 30780,                    \n",
    " 'HbA1c': 30750,\n",
    " 'ICD10': 41270,\n",
    " 'ICD9': 41271,\n",
    " 'accleration_average':90012,\n",
    " 'accleration_good_calibrate':90016,\n",
    " 'accleration_good_wear_time':90015,\n",
    " 'TV': 1070,\n",
    " 'age': 21022,\n",
    " 'alcohol': 1558,\n",
    " 'all_MET': 22040,\n",
    " 'c_reactive_protein': 30710,\n",
    " 'computer': 1080,\n",
    " 'cooked_vegie': 1289,\n",
    " 'education': 6138,\n",
    " 'fresh_fruit': 1309,\n",
    " 'job': 22601,\n",
    " 'moderate_activity': 894,\n",
    " 'process_meat': 1349,\n",
    " 'raw_vegie': 1299,\n",
    " 'sex': 31,\n",
    " 'sleep': 1160,\n",
    " 'smoke': 20116,\n",
    " 'total_cholesteral': 30690,\n",
    " 'triglycerides': 30870,\n",
    " 'townsend': 189,\n",
    " 'vigorous_activity': 914,\n",
    " 'walk': 874,\n",
    " 'day_walk': 864,\n",
    " 'day_moderate': 884,\n",
    " 'day_vigorous': 904,\n",
    " 'ethnic_background':21000\n",
    "}\n",
    "#was trying to change smoke from 20116 to 22506, but revert back due to high missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_paper_list=[\n",
    "\"age\",\n",
    "\"sex\",\n",
    "\"BMI\",\n",
    "\"alcohol\",\n",
    "\"sleep\",\n",
    "\"walk\",\n",
    "\"moderate_activity\",\n",
    "\"vigorous_activity\",\n",
    "\"TV\",\n",
    "\"computer\",\n",
    "\"smoke\",\n",
    "\"accleration_average\",\n",
    "'accleration_good_calibrate',\n",
    "'accleration_good_wear_time',\n",
    "\"townsend\",\n",
    "\"total_cholesteral\",\n",
    "\"ICD10\",\n",
    "\"ICD9\",\n",
    "\"day_vigorous\",\n",
    "\"day_moderate\",\n",
    "\"day_walk\",\n",
    "\"ethnic_background\",\n",
    "\"HDL\",\n",
    "\"LDL\",\n",
    "\"triglycerides\",\n",
    "\"HbA1c\",\n",
    "\"c_reactive_protein\",\n",
    "\"cooked_vegie\",\n",
    "\"raw_vegie\",\n",
    "\"fresh_fruit\",\n",
    "\"process_meat\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list=HF_paper_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names = [] \n",
    "for feature in feature_list:\n",
    "    print(feature)\n",
    "    print(field_names_for_id(feature_code_mapping[feature]))\n",
    "    field_names+=field_names_for_id(feature_code_mapping[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing fields into a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = dxdata.connect(dialect=\"hive+pyspark\")\n",
    "df = participant.retrieve_fields(engine=engine, names=field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashg_df_pandas=df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Use readable field name and save as pandas pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_to_remove = \"[-\\|\\,]\"\n",
    "title_dict={}\n",
    "for field_name in field_names:\n",
    "    new_name=participant[field_name].title\n",
    "    new_name = re.sub(characters_to_remove, \"\", new_name)\n",
    "    new_name = new_name.replace(' ','_')\n",
    "    new_name = new_name.replace('_/_','_')\n",
    "    new_name = re.sub(\"__+\", \"_\", new_name)\n",
    "    title_dict[field_name]=new_name\n",
    "title_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ashg_df_pandas.rename(columns=title_dict,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to pickle, so you don't have to use cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashg_df_pandas.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashg_df_pandas.to_pickle(\"./ashg_df_pandas.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dx mkdir ASHGdemo\n",
    "! dx upload ashg_df_pandas.pkl --path /ASHGdemo/ashg_df_pandas.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check in the project if pandas pickle file have been completely uploaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Close Spark and setup data/library in ML_IP instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Spark cluster and launch a Python_R instance with mem1_ssd1_v2_x8 instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done using UKB-RAP UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On new instance, install new version of Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn==0.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now restart kernel to make new Seaborn in effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from scipy import stats\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pandas pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dx download ASHGdemo/ashg_df_pandas.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashg_df_pandas=pd.read_pickle(\"./ashg_df_pandas.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashg_df_pandas.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Mark missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashg_df_pandas.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "1. There are both NaN and unexpected negative values. These values should be processed to have correct representation. \n",
    "2. Q1, Q2, Q3 shows that most of these features have discrete values. Some of them should be nominal and ordinal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashg_df_pandas.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashg_df_pandas.replace({\n",
    "    'Alcohol_intake_frequency._Instance_0':{-3:np.nan},\n",
    "    'Alcohol_intake_frequency._Instance_1':{-3:np.nan},\n",
    "    'Alcohol_intake_frequency._Instance_2':{-3:np.nan},\n",
    "    'Alcohol_intake_frequency._Instance_3':{-3:np.nan},\n",
    "    'Overall_health_rating_Instance_0':{-3:np.nan,-1:np.nan},\n",
    "    'Overall_health_rating_Instance_1':{-3:np.nan,-1:np.nan},\n",
    "    'Overall_health_rating_Instance_2':{-3:np.nan,-1:np.nan},\n",
    "    'Overall_health_rating_Instance_3':{-3:np.nan,-1:np.nan},\n",
    "    'Smoking_status_Instance_0':{-3:np.nan},\n",
    "    'Smoking_status_Instance_1':{-3:np.nan},\n",
    "    'Smoking_status_Instance_2':{-3:np.nan},\n",
    "    'Smoking_status_Instance_3':{-3:np.nan},\n",
    "    'Sleep_duration_Instance_0':{-3:np.nan,-1:np.nan},\n",
    "    'Sleep_duration_Instance_1':{-3:np.nan,-1:np.nan},\n",
    "    'Sleep_duration_Instance_2':{-3:np.nan,-1:np.nan},\n",
    "    'Sleep_duration_Instance_3':{-3:np.nan,-1:np.nan},\n",
    "    'Duration_of_walks_Instance_0':{-3:np.nan,-1:np.nan},\n",
    "    'Duration_of_walks_Instance_1':{-3:np.nan,-1:np.nan},\n",
    "    'Duration_of_walks_Instance_2':{-3:np.nan,-1:np.nan},\n",
    "    'Duration_of_walks_Instance_3':{-3:np.nan,-1:np.nan},\n",
    "    'Duration_of_moderate_activity_Instance_0':{-3:np.nan,-1:np.nan},\n",
    "    'Duration_of_moderate_activity_Instance_1':{-3:np.nan,-1:np.nan},\n",
    "    'Duration_of_moderate_activity_Instance_2':{-3:np.nan,-1:np.nan},\n",
    "    'Duration_of_moderate_activity_Instance_3':{-3:np.nan,-1:np.nan},\n",
    "    'Duration_of_vigorous_activity_Instance_0':{-3:np.nan,-1:np.nan},\n",
    "    'Duration_of_vigorous_activity_Instance_1':{-3:np.nan,-1:np.nan},\n",
    "    'Duration_of_vigorous_activity_Instance_2':{-3:np.nan,-1:np.nan},\n",
    "    'Duration_of_vigorous_activity_Instance_3':{-3:np.nan,-1:np.nan},\n",
    "    'Time_spent_watching_television_(TV)_Instance_0':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Time_spent_watching_television_(TV)_Instance_1':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Time_spent_watching_television_(TV)_Instance_2':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Time_spent_watching_television_(TV)_Instance_3':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Time_spent_using_computer_Instance_0':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Time_spent_using_computer_Instance_1':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Time_spent_using_computer_Instance_2':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Time_spent_using_computer_Instance_3':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Number_of_days/week_of_vigorous_physical_activity_10+_minutes_Instance_0':{-3:np.nan,-1:np.nan},\n",
    "    'Number_of_days/week_of_vigorous_physical_activity_10+_minutes_Instance_1':{-3:np.nan,-1:np.nan},\n",
    "    'Number_of_days/week_of_vigorous_physical_activity_10+_minutes_Instance_2':{-3:np.nan,-1:np.nan},\n",
    "    'Number_of_days/week_of_vigorous_physical_activity_10+_minutes_Instance_3':{-3:np.nan,-1:np.nan},\n",
    "    'Number_of_days/week_of_moderate_physical_activity_10+_minutes_Instance_0':{-3:np.nan,-1:np.nan},\n",
    "    'Number_of_days/week_of_moderate_physical_activity_10+_minutes_Instance_1':{-3:np.nan,-1:np.nan},\n",
    "    'Number_of_days/week_of_moderate_physical_activity_10+_minutes_Instance_2':{-3:np.nan,-1:np.nan},\n",
    "    'Number_of_days/week_of_moderate_physical_activity_10+_minutes_Instance_3':{-3:np.nan,-1:np.nan},\n",
    "    'Number_of_days/week_walked_10+_minutes_Instance_0':{-3:np.nan,-2:np.nan,-1:np.nan},\n",
    "    'Number_of_days/week_walked_10+_minutes_Instance_1':{-3:np.nan,-2:np.nan,-1:np.nan},\n",
    "    'Number_of_days/week_walked_10+_minutes_Instance_2':{-3:np.nan,-2:np.nan,-1:np.nan},\n",
    "    'Number_of_days/week_walked_10+_minutes_Instance_3':{-3:np.nan,-2:np.nan,-1:np.nan},\n",
    "    'Ethnic_background_Instance_0':{-1:np.nan,-3:np.nan},\n",
    "    'Ethnic_background_Instance_1':{-1:np.nan,-3:np.nan},\n",
    "    'Ethnic_background_Instance_2':{-1:np.nan,-3:np.nan},\n",
    "    'Cooked_vegetable_intake_Instance_0':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Cooked_vegetable_intake_Instance_1':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Cooked_vegetable_intake_Instance_2':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Cooked_vegetable_intake_Instance_3':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Salad_raw_vegetable_intake_Instance_0':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Salad_raw_vegetable_intake_Instance_1':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Salad_raw_vegetable_intake_Instance_2':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Salad_raw_vegetable_intake_Instance_3':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Fresh_fruit_intake_Instance_0':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Fresh_fruit_intake_Instance_1':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Fresh_fruit_intake_Instance_2':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Fresh_fruit_intake_Instance_3':{-3:np.nan,-1:np.nan,-10:0},\n",
    "    'Processed_meat_intake_Instance_0':{-3:np.nan,-1:np.nan},\n",
    "    'Processed_meat_intake_Instance_1':{-3:np.nan,-1:np.nan},\n",
    "    'Processed_meat_intake_Instance_2':{-3:np.nan,-1:np.nan},\n",
    "    'Processed_meat_intake_Instance_3':{-3:np.nan,-1:np.nan},\n",
    "                     },inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Overall_acceleration_average based on two other field which indicate low data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashg_df_pandas['Overall_acceleration_average'][(ashg_df_pandas['Data_quality_good_calibration'] != 1) | (ashg_df_pandas['Data_quality_good_wear_time'] != 1)]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashg_df_pandas.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Process variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Combine multiple-instance (multiple-measured) features\n",
    "\n",
    "How to combine multiple measured features is a research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk=np.nanmean(np.array([ashg_df_pandas['Number_of_days/week_walked_10+_minutes_Instance_0']*ashg_df_pandas['Duration_of_walks_Instance_0'],\n",
    "                    ashg_df_pandas['Number_of_days/week_walked_10+_minutes_Instance_1']*ashg_df_pandas['Duration_of_walks_Instance_1'],\n",
    "                    ashg_df_pandas['Number_of_days/week_walked_10+_minutes_Instance_2']*ashg_df_pandas['Duration_of_walks_Instance_2'],\n",
    "                    ashg_df_pandas['Number_of_days/week_walked_10+_minutes_Instance_3']*ashg_df_pandas['Duration_of_walks_Instance_3']\n",
    "                   ]),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderate=np.nanmean(np.array([ashg_df_pandas['Number_of_days/week_of_moderate_physical_activity_10+_minutes_Instance_0']*ashg_df_pandas['Duration_of_moderate_activity_Instance_0'],\n",
    "                              ashg_df_pandas['Number_of_days/week_of_moderate_physical_activity_10+_minutes_Instance_1']*ashg_df_pandas['Duration_of_moderate_activity_Instance_1'],\n",
    "                              ashg_df_pandas['Number_of_days/week_of_moderate_physical_activity_10+_minutes_Instance_2']*ashg_df_pandas['Duration_of_moderate_activity_Instance_2'],\n",
    "                              ashg_df_pandas['Number_of_days/week_of_moderate_physical_activity_10+_minutes_Instance_3']*ashg_df_pandas['Duration_of_moderate_activity_Instance_3'],\n",
    "                             ]),axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vigorous=np.nanmean(np.array([ashg_df_pandas['Number_of_days/week_of_vigorous_physical_activity_10+_minutes_Instance_0']*ashg_df_pandas['Duration_of_vigorous_activity_Instance_0'],\n",
    "                              ashg_df_pandas['Number_of_days/week_of_vigorous_physical_activity_10+_minutes_Instance_1']*ashg_df_pandas['Duration_of_vigorous_activity_Instance_1'],\n",
    "                              ashg_df_pandas['Number_of_days/week_of_vigorous_physical_activity_10+_minutes_Instance_2']*ashg_df_pandas['Duration_of_vigorous_activity_Instance_2'],\n",
    "                              ashg_df_pandas['Number_of_days/week_of_vigorous_physical_activity_10+_minutes_Instance_3']*ashg_df_pandas['Duration_of_vigorous_activity_Instance_3'],\n",
    "                             ]),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi_average=ashg_df_pandas[['Body_mass_index_(BMI)_Instance_0','Body_mass_index_(BMI)_Instance_1','Body_mass_index_(BMI)_Instance_2','Body_mass_index_(BMI)_Instance_3']].mean(skipna=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_return(dataset):\n",
    "    output=dataset.dropna().unique()\n",
    "    if len(output)==1:\n",
    "        return output[0]\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnic=ashg_df_pandas[['Ethnic_background_Instance_0','Ethnic_background_Instance_1','Ethnic_background_Instance_2']].apply(one_return,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caucasian=ethnic.map(lambda x: 1 if (str(x).startswith('1')) else 0)\n",
    "caucasian2=caucasian.astype('category')\n",
    "caucasian2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caucasian=caucasian2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnic2=ethnic.astype('category')\n",
    "ethnic2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep=ashg_df_pandas[['Sleep_duration_Instance_0','Sleep_duration_Instance_1','Sleep_duration_Instance_2','Sleep_duration_Instance_3']].mean(skipna=True,axis=1)\n",
    "alcohol=ashg_df_pandas[['Alcohol_intake_frequency._Instance_0','Alcohol_intake_frequency._Instance_1','Alcohol_intake_frequency._Instance_2','Alcohol_intake_frequency._Instance_3',]].mean(skipna=True,axis=1)\n",
    "tv=ashg_df_pandas[['Time_spent_watching_television_(TV)_Instance_0','Time_spent_watching_television_(TV)_Instance_1','Time_spent_watching_television_(TV)_Instance_2','Time_spent_watching_television_(TV)_Instance_3',]].mean(skipna=True,axis=1)\n",
    "computer=ashg_df_pandas[['Time_spent_using_computer_Instance_0','Time_spent_using_computer_Instance_1','Time_spent_using_computer_Instance_2','Time_spent_using_computer_Instance_3',]].mean(skipna=True,axis=1)\n",
    "smoking=ashg_df_pandas[['Smoking_status_Instance_0','Smoking_status_Instance_1','Smoking_status_Instance_2','Smoking_status_Instance_3',]].mean(skipna=True,axis=1)\n",
    "cholesterol=ashg_df_pandas[['Cholesterol_Instance_0','Cholesterol_Instance_1']].mean(skipna=True,axis=1)\n",
    "HDL=ashg_df_pandas[['HDL_cholesterol_Instance_0','HDL_cholesterol_Instance_1']].mean(skipna=True,axis=1)\n",
    "LDL=ashg_df_pandas[['LDL_direct_Instance_0','LDL_direct_Instance_1']].mean(skipna=True,axis=1)\n",
    "triglyceride=ashg_df_pandas[['Triglycerides_Instance_0','Triglycerides_Instance_1']].mean(skipna=True,axis=1)\n",
    "c_reactive=ashg_df_pandas[['Creactive_protein_Instance_0','Creactive_protein_Instance_1']].mean(skipna=True,axis=1)\n",
    "HbA1c=ashg_df_pandas[['Glycated_haemoglobin_(HbA1c)_Instance_0','Glycated_haemoglobin_(HbA1c)_Instance_1']].mean(skipna=True,axis=1)\n",
    "raw_vegie=ashg_df_pandas[['Salad_raw_vegetable_intake_Instance_0','Salad_raw_vegetable_intake_Instance_1','Salad_raw_vegetable_intake_Instance_2','Salad_raw_vegetable_intake_Instance_3']].mean(skipna=True,axis=1)\n",
    "cook_vegie=ashg_df_pandas[['Cooked_vegetable_intake_Instance_0','Cooked_vegetable_intake_Instance_1','Cooked_vegetable_intake_Instance_2','Cooked_vegetable_intake_Instance_3']].mean(skipna=True,axis=1)\n",
    "fresh_fruit=ashg_df_pandas[['Fresh_fruit_intake_Instance_0','Fresh_fruit_intake_Instance_1','Fresh_fruit_intake_Instance_2','Fresh_fruit_intake_Instance_3']].mean(skipna=True,axis=1)\n",
    "process_meat=ashg_df_pandas[['Processed_meat_intake_Instance_0','Processed_meat_intake_Instance_1','Processed_meat_intake_Instance_2','Processed_meat_intake_Instance_3']].mean(skipna=True,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make reverse of normal alcohol to high number mean consuming more alcohol\n",
    "alcohol_intake=6-alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alcohol.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alcohol_intake.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Create label, bin variables, cast categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_HF_label(row):\n",
    "    ICD_10_HF_list=['I500','I501','I509']\n",
    "    ICD_9_HF_list=['4280','4281']\n",
    "    ICD10=row.Diagnoses_ICD10\n",
    "    ICD9=row.Diagnoses_ICD9\n",
    "    if not ICD10 and not ICD9:\n",
    "        # I encode no disease as missing data historically, but change to 0 after found that people with missing data is healthier (Data-Field 2178)\n",
    "        # There is 'None', but no empty list in original dataset\n",
    "        return 0\n",
    "    if ICD10:\n",
    "        if any(item in ICD10 for item in ICD_10_HF_list):\n",
    "            return 1\n",
    "    if ICD9:\n",
    "        if any(item in ICD9 for item in ICD_9_HF_list):\n",
    "            return 1\n",
    "\n",
    "    return 0\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=ashg_df_pandas.apply(create_HF_label, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_townsend(value):\n",
    "    if value <=-3.97:\n",
    "        return 0\n",
    "    elif value <=-2.84:\n",
    "        return 1\n",
    "    elif value <=-1.44:\n",
    "        return 2\n",
    "    elif value <=1.11:\n",
    "        return 3\n",
    "    elif value >1.11:\n",
    "        return 4\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_acceleration(value):\n",
    "    if value <=25:\n",
    "        return 0\n",
    "    elif value <=100:\n",
    "        return 1\n",
    "    elif value <=425:\n",
    "        return 2\n",
    "    elif value >425:\n",
    "        return 3\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "townsend_grade=ashg_df_pandas['Townsend_deprivation_index_at_recruitment'].map(convert_townsend)\n",
    "acceleration_grade=ashg_df_pandas['Overall_acceleration_average'].map(convert_acceleration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashg_df_pandas['label']=label\n",
    "ashg_df_pandas['townsend_grade']=townsend_grade\n",
    "ashg_df_pandas['acceleration_grade']=acceleration_grade\n",
    "ashg_df_pandas['label']=ashg_df_pandas['label'].astype('category')\n",
    "ashg_df_pandas['Sex']=ashg_df_pandas['Sex'].astype('category')\n",
    "ashg_df_pandas.drop(columns=['Diagnoses_ICD10','Diagnoses_ICD9','eid'],inplace=True)\n",
    "ashg_df_pandas['caucasian']=caucasian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bmi(value):\n",
    "    if value <=25:\n",
    "        return 0\n",
    "    elif value <=30:\n",
    "        return 1\n",
    "    elif value <=35:\n",
    "        return 2\n",
    "    elif value >35:\n",
    "        return 3\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashg_df_pandas['walk']=walk\n",
    "ashg_df_pandas['moderate']=moderate\n",
    "ashg_df_pandas['vigorous']=vigorous\n",
    "ashg_df_pandas['bmi_grade']=bmi_average.map(convert_bmi)\n",
    "ashg_df_pandas['bmi_average']=bmi_average\n",
    "ashg_df_pandas['sleep']=sleep\n",
    "ashg_df_pandas['alcohol']=alcohol_intake\n",
    "ashg_df_pandas['tv']=tv\n",
    "ashg_df_pandas['computer']=computer\n",
    "ashg_df_pandas['smoking']=smoking\n",
    "ashg_df_pandas['cholesterol']=cholesterol\n",
    "ashg_df_pandas['HDL']=HDL\n",
    "ashg_df_pandas['LDL']=LDL\n",
    "ashg_df_pandas['triglyceride']=triglyceride\n",
    "ashg_df_pandas['c_reactive']=c_reactive\n",
    "ashg_df_pandas['HbA1c']=HbA1c\n",
    "ashg_df_pandas['raw_vegie']=raw_vegie\n",
    "ashg_df_pandas['cook_vegie']=cook_vegie\n",
    "ashg_df_pandas['fresh_fruit']=fresh_fruit\n",
    "ashg_df_pandas['process_meat']=process_meat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashg_df_pandas['bmi_grade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashg_df_pandas.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Remove individual with rarely missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed=ashg_df_pandas.dropna(subset=['Sex','Age_at_recruitment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Drop unused fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed=hf_df_processed.drop(columns=[\n",
    "                           'Body_mass_index_(BMI)_Instance_0','Body_mass_index_(BMI)_Instance_1','Body_mass_index_(BMI)_Instance_2','Body_mass_index_(BMI)_Instance_3',\n",
    "                           'Number_of_days/week_of_vigorous_physical_activity_10+_minutes_Instance_0','Number_of_days/week_of_vigorous_physical_activity_10+_minutes_Instance_1',\n",
    "                           'Number_of_days/week_of_vigorous_physical_activity_10+_minutes_Instance_2','Number_of_days/week_of_vigorous_physical_activity_10+_minutes_Instance_3',\n",
    "                           'Duration_of_vigorous_activity_Instance_0','Duration_of_vigorous_activity_Instance_1','Duration_of_vigorous_activity_Instance_2','Duration_of_vigorous_activity_Instance_3',\n",
    "                           'Number_of_days/week_of_moderate_physical_activity_10+_minutes_Instance_0','Number_of_days/week_of_moderate_physical_activity_10+_minutes_Instance_1',\n",
    "                           'Number_of_days/week_of_moderate_physical_activity_10+_minutes_Instance_2','Number_of_days/week_of_moderate_physical_activity_10+_minutes_Instance_3',\n",
    "                           'Duration_of_moderate_activity_Instance_0','Duration_of_moderate_activity_Instance_1','Duration_of_moderate_activity_Instance_2','Duration_of_moderate_activity_Instance_3',\n",
    "                           'Number_of_days/week_walked_10+_minutes_Instance_0','Number_of_days/week_walked_10+_minutes_Instance_1','Number_of_days/week_walked_10+_minutes_Instance_2','Number_of_days/week_walked_10+_minutes_Instance_3',\n",
    "                           'Duration_of_walks_Instance_0','Duration_of_walks_Instance_1','Duration_of_walks_Instance_2','Duration_of_walks_Instance_3',\n",
    "                           'Sleep_duration_Instance_0','Sleep_duration_Instance_1','Sleep_duration_Instance_2','Sleep_duration_Instance_3',\n",
    "                           'Alcohol_intake_frequency._Instance_0','Alcohol_intake_frequency._Instance_1','Alcohol_intake_frequency._Instance_2','Alcohol_intake_frequency._Instance_3',\n",
    "                           'Time_spent_watching_television_(TV)_Instance_0','Time_spent_watching_television_(TV)_Instance_1','Time_spent_watching_television_(TV)_Instance_2','Time_spent_watching_television_(TV)_Instance_3',\n",
    "                           'Time_spent_using_computer_Instance_0','Time_spent_using_computer_Instance_1','Time_spent_using_computer_Instance_2','Time_spent_using_computer_Instance_3',\n",
    "                           'Smoking_status_Instance_0','Smoking_status_Instance_1','Smoking_status_Instance_2','Smoking_status_Instance_3',\n",
    "                           'Cholesterol_Instance_0','Cholesterol_Instance_1','Data_quality_good_calibration','Data_quality_good_wear_time',\n",
    "                           'HDL_cholesterol_Instance_0','HDL_cholesterol_Instance_1',\n",
    "                           'LDL_direct_Instance_0','LDL_direct_Instance_1',\n",
    "                           'Triglycerides_Instance_0','Triglycerides_Instance_1',\n",
    "                           'Creactive_protein_Instance_0','Creactive_protein_Instance_1',\n",
    "                           'Glycated_haemoglobin_(HbA1c)_Instance_0','Glycated_haemoglobin_(HbA1c)_Instance_1',\n",
    "                           'Salad_raw_vegetable_intake_Instance_0','Salad_raw_vegetable_intake_Instance_1','Salad_raw_vegetable_intake_Instance_2','Salad_raw_vegetable_intake_Instance_3',\n",
    "                           'Cooked_vegetable_intake_Instance_0','Cooked_vegetable_intake_Instance_1','Cooked_vegetable_intake_Instance_2','Cooked_vegetable_intake_Instance_3',\n",
    "                           'Fresh_fruit_intake_Instance_0','Fresh_fruit_intake_Instance_1','Fresh_fruit_intake_Instance_2','Fresh_fruit_intake_Instance_3',\n",
    "                           'Processed_meat_intake_Instance_0','Processed_meat_intake_Instance_1','Processed_meat_intake_Instance_2','Processed_meat_intake_Instance_3',\n",
    "                           'Ethnic_background_Instance_0','Ethnic_background_Instance_1','Ethnic_background_Instance_2'\n",
    "                          ],inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_count=hf_df_processed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_count.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Univariate analyses\n",
    "\n",
    "Inspect effect of individual feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.groupby('label').agg(['median','mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check self-reported PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.groupby('label')[['walk','moderate','vigorous']].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.groupby('label')[['walk','moderate','vigorous']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"walk\",\n",
    "                 data=hf_df_processed)\n",
    "ax.set(ylim=(0, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"moderate\",\n",
    "                 data=hf_df_processed)\n",
    "ax.set(ylim=(0, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"vigorous\",\n",
    "                 data=hf_df_processed)\n",
    "ax.set(ylim=(0, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check mean acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.groupby('label')['Overall_acceleration_average'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"Overall_acceleration_average\",\n",
    "                 data=hf_df_processed)\n",
    "ax.set(ylim=(0, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.groupby('label')[['Age_at_recruitment']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"Age_at_recruitment\",\n",
    "                 data=hf_df_processed)\n",
    "ax.set(ylim=(0, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.groupby('label')['Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_count=hf_df_processed.groupby('label')['Sex'].value_counts()\n",
    "sum_noHF=sum(gender_count[0])\n",
    "sum_HF=sum(gender_count[1])\n",
    "normalize_HF_by_grade=gender_count[1]/sum_HF\n",
    "normalize_noHF_by_grade=gender_count[0]/sum_noHF\n",
    "normalize_noHF_by_grade.sort_index(inplace=True)\n",
    "normalize_HF_by_grade.sort_index(inplace=True)\n",
    "fig, ax = plt.subplots()\n",
    "width = 0.35  # the width of the bars\n",
    "labels=['Female','Male']\n",
    "positions=[0,1]\n",
    "x=np.arange(len(labels))\n",
    "rects1 = ax.bar(x - width/2, normalize_noHF_by_grade, width, label='noHF')\n",
    "rects2 = ax.bar(x + width/2, normalize_HF_by_grade, width, label='HF')\n",
    "#ax.bar(normalize_noHF_by_grade)\n",
    "#ax.bar(normalize_HF_by_grade)\n",
    "ax.legend()\n",
    "plt.xticks(positions, labels)\n",
    "plt.ylabel('Ratio')\n",
    "plt.xlabel('Gender')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.groupby('label')['bmi_average'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.groupby('label')[['bmi_average']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"bmi_average\",\n",
    "                 data=hf_df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Townsend deprivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"Townsend_deprivation_index_at_recruitment\",\n",
    "                 data=hf_df_processed)\n",
    "#ax.set(ylim=(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cholesterol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"cholesterol\",\n",
    "                 data=hf_df_processed)\n",
    "ax.set(ylim=(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"tv\",\n",
    "                 data=hf_df_processed)\n",
    "ax.set(ylim=(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"computer\",\n",
    "                 data=hf_df_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"walk\",\n",
    "                 data=hf_df_processed)\n",
    "#ax.set(ylim=(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"sleep\",\n",
    "                 data=hf_df_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"alcohol\",\n",
    "                 data=hf_df_processed)\n",
    "ax.set(ylim=(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"smoking\",\n",
    "                 data=hf_df_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blood test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"HDL\",\n",
    "                 data=hf_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"LDL\",\n",
    "                 data=hf_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"triglyceride\",\n",
    "                 data=hf_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"c_reactive\",\n",
    "                 data=hf_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"HbA1c\",\n",
    "                 data=hf_df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"raw_vegie\",\n",
    "                 data=hf_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"cook_vegie\",\n",
    "                 data=hf_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"fresh_fruit\",\n",
    "                 data=hf_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"label\", y=\"process_meat\",\n",
    "                 data=hf_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed=hf_df_processed.drop(columns=[\n",
    "    'townsend_grade','acceleration_grade','bmi_grade'\n",
    "                          ],inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_count=hf_df_processed.isna().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*missing_value_count.sort_values()/hf_df_processed.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We gonna remove Overall_acceleration_average later for multivariate or ML analysis due to its high missing value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to csv for R and other stat analyhsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.to_csv('hf_df_processed.csv',na_rep='NA',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dx upload hf_df_processed.csv  --path /ASHGdemo/hf_df_processed.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once again, we may start from hf_df_processed.csv going foward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here is how to load data as CSV\n",
    "\n",
    "`hf_df_processed = pd.read_csv('hf_df_processed.csv', sep=',')`\n",
    "\n",
    "#### check datatype\n",
    "\n",
    "`hf_df_processed.info()`\n",
    "\n",
    "#### change to use categorical\n",
    "\n",
    "`hf_df_processed['label']=hf_df_processed['label'].astype('category')`\n",
    "\n",
    "`hf_df_processed['Sex']=hf_df_processed['Sex'].astype('category')`\n",
    "\n",
    "`hf_df_processed['caucasian']=hf_df_processed['caucasian'].astype('category')`\n",
    "\n",
    "#### check datatype again\n",
    "\n",
    "`hf_df_processed.info()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Basic machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install all modules we will use and import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn==0.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install interpret==0.2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## restart kernal here and import all the package we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import json\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.provider import InlineProvider\n",
    "from interpret import set_visualize_provider\n",
    "\n",
    "set_visualize_provider(InlineProvider())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret import show\n",
    "from interpret.data import ClassHistogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret import set_show_addr, get_show_addr \n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dx download /ASHGdemo/hf_df_processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed = pd.read_csv('hf_df_processed.csv', sep=',')\n",
    "hf_df_processed['label']=hf_df_processed['label'].astype('category')\n",
    "hf_df_processed['Sex']=hf_df_processed['Sex'].astype('category')\n",
    "hf_df_processed['caucasian']=hf_df_processed['caucasian'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.dropna(subset=['label'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.isna().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_processed.isna().sum().sort_values()*100/(hf_df_processed.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove feature with high missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_good_measure=hf_df_processed.drop(columns=['Overall_acceleration_average'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate between full data set and dataset with no missing value(Complete Case Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df_good_measure.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CCA_hf_df_good_measure=hf_df_good_measure.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CCA_hf_df_good_measure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CCA_hf_df_good_measure.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use downsampling to balance class and separate out label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hf_df_good_measure.drop(columns=['label'])\n",
    "y = hf_df_good_measure['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y)\n",
    "estimate = counter[0] / counter[1]\n",
    "estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CCA_X = CCA_hf_df_good_measure.drop(columns=['label'])\n",
    "CCA_y = CCA_hf_df_good_measure['label']\n",
    "counter = Counter(CCA_y)\n",
    "CCA_estimate = counter[0] / counter[1]   \n",
    "CCA_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=0)\n",
    "CCA_X_resampled, CCA_y_resampled = rus.fit_resample(CCA_X, CCA_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CCA_y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hot encoding categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols=['Sex','caucasian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_resampled , y_resampled , train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CCA_X_train, CCA_X_valid, CCA_y_train, CCA_y_valid = train_test_split(CCA_X_resampled , CCA_y_resampled , train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(CCA_X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(CCA_X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = CCA_X_train.index\n",
    "OH_cols_valid.index = CCA_X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = CCA_X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = CCA_X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_CCA_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_CCA_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run multiple ML prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run models in the following order:\n",
    "\n",
    "    * XGboosted on full dataset\n",
    "    * XGboosted on CCA (no missing value)\n",
    "    * lightGBM on full dataset\n",
    "    * lightGBM on CCA\n",
    "    * random forest on CCA\n",
    "    * logistic regression on CCA\n",
    "    * Explaininable boosting machine (EBM) on CCA\n",
    "\n",
    "Note that random forest, logistic regression, and EBM can't handle missing values, so we only use them on CCA.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_full= XGBClassifier(n_estimators=1000)\n",
    "xgb_full.fit(OH_X_train,y_train,early_stopping_rounds=5,\n",
    "            eval_set=[(OH_X_valid, y_valid)],eval_metric=['auc','logloss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_CCA= XGBClassifier(n_estimators=1000)\n",
    "xgb_CCA.fit(OH_CCA_X_train,CCA_y_train,early_stopping_rounds=5,\n",
    "            eval_set=[(OH_CCA_X_valid, CCA_y_valid)],eval_metric=['auc','logloss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_full=LGBMClassifier(n_estimators=1000)\n",
    "lgbm_full.fit(OH_X_train,y_train,early_stopping_rounds=5,\n",
    "            eval_set=[(OH_X_valid, y_valid)],eval_metric=['auc','logloss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_CCA=LGBMClassifier(n_estimators=1000)\n",
    "lgbm_CCA.fit(OH_CCA_X_train,CCA_y_train,early_stopping_rounds=5,\n",
    "            eval_set=[(OH_CCA_X_valid, CCA_y_valid)],eval_metric=['auc','logloss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_CCA = RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "forest_CCA.fit(OH_CCA_X_train,CCA_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_CCA=LogisticRegression(random_state=0,solver='lbfgs',max_iter=10000)# solver='saga')\n",
    "logistic_CCA.fit(OH_CCA_X_train,CCA_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_CCA = ExplainableBoostingClassifier(random_state=127,max_rounds=5000)\n",
    "ebm_CCA.fit(CCA_X_train, CCA_y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the following cells to analyze accuracy and F1 score in test and testing dataset for models that run on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full=lgbm_full\n",
    "preds_train = model_full.predict(OH_X_train)\n",
    "preds_val = model_full.predict(OH_X_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train=accuracy_score(y_train,preds_train)\n",
    "accuracy_val=accuracy_score(y_valid,preds_val)\n",
    "f1_train=f1_score(y_train,preds_train)\n",
    "f1_val=f1_score(y_valid,preds_val)\n",
    "\n",
    "print('predicted train {}'.format(Counter(preds_train)))\n",
    "print('predicted valid {}'.format(Counter(preds_val)))\n",
    "print('Train accuracy {} and F1 {}'.format(accuracy_train,f1_train))\n",
    "print('Validation accuracy {} and F1 {}'.format(accuracy_val,f1_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the following cells to analyze accuracy and F1 score in test and testing dataset for models that run on CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CCA=lgbm_CCA\n",
    "preds_train = model_CCA.predict(OH_CCA_X_train)\n",
    "preds_val = model_CCA.predict(OH_CCA_X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train=accuracy_score(CCA_y_train,preds_train)\n",
    "accuracy_val=accuracy_score(CCA_y_valid,preds_val)\n",
    "f1_train=f1_score(CCA_y_train,preds_train)\n",
    "f1_val=f1_score(CCA_y_valid,preds_val)\n",
    "\n",
    "print('predicted train {}'.format(Counter(preds_train)))\n",
    "print('predicted valid {}'.format(Counter(preds_val)))\n",
    "print('Train accuracy {} and F1 {}'.format(accuracy_train,f1_train))\n",
    "print('Validation accuracy {} and F1 {}'.format(accuracy_val,f1_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The exception is EBM which we dont' need one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = ebm_CCA.predict(CCA_X_train)\n",
    "preds_val = ebm_CCA.predict(CCA_X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are results we get when make poster"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## weight class approach\n",
    "#xgb_full 1000\n",
    "[15]\tvalidation_0-auc:0.79715\tvalidation_0-logloss:0.54618\n",
    "[16]\tvalidation_0-auc:0.79726\tvalidation_0-logloss:0.54622\n",
    "[17]\tvalidation_0-auc:0.79731\tvalidation_0-logloss:0.54628\n",
    "[18]\tvalidation_0-auc:0.79734\tvalidation_0-logloss:0.54635\n",
    "[19]\tvalidation_0-auc:0.79751\tvalidation_0-logloss:0.54639\n",
    "Train accuracy 0.7657222665602554 and F1 0.7677518889196567\n",
    "Validation accuracy 0.7239067985955953 and F1 0.7307812013694367\n",
    "#xbg_CCA 1000\n",
    "[9]\tvalidation_0-auc:0.78878\tvalidation_0-logloss:0.55369\n",
    "[10]\tvalidation_0-auc:0.78793\tvalidation_0-logloss:0.55500\n",
    "[11]\tvalidation_0-auc:0.78853\tvalidation_0-logloss:0.55437\n",
    "[12]\tvalidation_0-auc:0.78887\tvalidation_0-logloss:0.55393\n",
    "[13]\tvalidation_0-auc:0.78891\tvalidation_0-logloss:0.55449\n",
    "Train accuracy 0.7905454545454546 and F1 0.7918473547267997\n",
    "Validation accuracy 0.7201861547411286 and F1 0.7280949689089881\n",
    "#lgbm_full 1000\n",
    "Train accuracy 0.7447725458898643 and F1 0.7503902591320638\n",
    "Validation accuracy 0.7285349505266517 and F1 0.7380255659941475\n",
    "Early stopping, best iteration is:\n",
    "[27]\tvalid_0's auc: 0.802062\tvalid_0's binary_logloss: 0.543302\n",
    "#lgbm_CCA 1000\n",
    "Train accuracy 0.7802181818181818 and F1 0.7819310145764181\n",
    "Validation accuracy 0.7242582897033158 and F1 0.734006734006734\n",
    "Early stopping, best iteration is:\n",
    "[30]\tvalid_0's auc: 0.79488\tvalid_0's binary_logloss: 0.547079\n",
    "#forest_CCA 100\n",
    "Train accuracy 1.0 and F1 1.0\n",
    "Validation accuracy 0.7201861547411286 and F1 0.7293190770962296\n",
    "#logistic_CCA 10000\n",
    "Train accuracy 0.7168 and F1 0.7199769883503524\n",
    "Validation accuracy 0.7306573589296103 and F1 0.7364826408651111\n",
    "#EBM 5000\n",
    "Train accuracy 0.7415272727272727 and F1 0.7419026870007261\n",
    "Validation accuracy 0.7242582897033158 and F1 0.7300683371298406\n",
    "#ensembl forest 100\n",
    "Train accuracy 0.7163274225774225 and F1 0.13174842815372562\n",
    "Validation accuracy 0.7039710289710289 and F1 0.09359944941500345\n",
    "# logistic_CCA noOH 10000\n",
    "Train accuracy 0.7160727272727273 and F1 0.719055843408175\n",
    "Validation accuracy 0.7300756253635835 and F1 0.7357630979498861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Interpretable and explainable ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shap.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will run SHAP on all model to investigate order of feature importance and their affects on each individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_xgb_full = shap.TreeExplainer(xgb_full)\n",
    "shap_values_xgb_full = explainer_xgb_full.shap_values(OH_X_valid,approximate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_xgb_full, OH_X_valid,plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_xgb_full, OH_X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_xgb_CCA = shap.TreeExplainer(xgb_CCA)\n",
    "shap_values_xgb_CCA = explainer_xgb_CCA.shap_values(OH_CCA_X_train,approximate=True)\n",
    "shap.summary_plot(shap_values_xgb_CCA, OH_CCA_X_train,plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_xgb_CCA, OH_CCA_X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_xgb_CCA2 = shap.TreeExplainer(xgb_CCA)\n",
    "shap_values_xgb_CCA2 = explainer_xgb_CCA2.shap_values(OH_CCA_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_xgb_CCA2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_xgb_CCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_lgbm_full = shap.TreeExplainer(lgbm_full)\n",
    "shap_values_lgbm_full = explainer_lgbm_full.shap_values(OH_X_valid)\n",
    "shap.summary_plot(shap_values_lgbm_full[1], OH_X_valid,plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_lgbm_full[1], OH_X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The lbgm_full has the best performance on test dataset, so we will also investigate partial dependent plot to see overall trend and scale of impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X100 = shap.utils.sample(OH_X_valid, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OH_X_valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.partial_dependence(\n",
    "    \"Age_at_recruitment\", lgbm_full.predict, X100, ice=False,\n",
    "    model_expected_value=True, feature_expected_value=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.partial_dependence(\n",
    "    \"bmi_average\", lgbm_full.predict, X100, ice=False,\n",
    "    model_expected_value=True, feature_expected_value=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.partial_dependence(\n",
    "    \"HbA1c\", lgbm_full.predict, X100, ice=False,\n",
    "    model_expected_value=True, feature_expected_value=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.partial_dependence(\n",
    "    \"cholesterol\", lgbm_full.predict, X100, ice=False,\n",
    "    model_expected_value=True, feature_expected_value=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.partial_dependence(\n",
    "    -4, lgbm_full.predict, X100, ice=False,\n",
    "    model_expected_value=True, feature_expected_value=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it doesn't understand categorical feature and pick a different kind of plotting. Even though this is compresible, it would be better to use different visuzliation for categorical from numerical. We will address this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_lgbm_CCA = shap.TreeExplainer(lgbm_CCA)\n",
    "shap_values_lgbm_CCA = explainer_lgbm_CCA.shap_values(OH_CCA_X_train)\n",
    "shap.summary_plot(shap_values_lgbm_CCA[1], OH_CCA_X_train,plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_lgbm_CCA[1], OH_CCA_X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(shap_values_lgbm_CCA))\n",
    "shap_values_lgbm_CCA[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_forest_CCA = shap.TreeExplainer(forest_CCA)\n",
    "shap_values_forest_CCA = explainer_forest_CCA.shap_values(OH_CCA_X_valid,approximate=True)\n",
    "shap.summary_plot(shap_values_forest_CCA[1], OH_CCA_X_valid,plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_forest_CCA[1], OH_CCA_X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_logistic_CCA = shap.Explainer(logistic_CCA,OH_CCA_X_valid)\n",
    "shap_values_logistic_CCA = explainer_logistic_CCA(OH_CCA_X_valid)\n",
    "shap.summary_plot(shap_values_logistic_CCA, OH_CCA_X_valid,plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(list(logistic_CCA.coef_[0]),list(OH_CCA_X_valid.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding for logis is hard to interpret. It is confusing which number represent what. Also, how come the pair number (0 vs 1 and 2 vs 3) have different value. We will try with non-one hot encoding instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_CCA_noOH=LogisticRegression(random_state=0,solver='lbfgs',max_iter=10000)# solver='saga')\n",
    "logistic_CCA_noOH.fit(CCA_X_train,CCA_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = logistic_CCA_noOH.predict(CCA_X_train)\n",
    "preds_val = logistic_CCA_noOH.predict(CCA_X_valid)\n",
    "accuracy_train=accuracy_score(CCA_y_train,preds_train)\n",
    "accuracy_val=accuracy_score(CCA_y_valid,preds_val)\n",
    "f1_train=f1_score(CCA_y_train,preds_train)\n",
    "f1_val=f1_score(CCA_y_valid,preds_val)\n",
    "print('predicted train {}'.format(Counter(preds_train)))\n",
    "print('predicted valid {}'.format(Counter(preds_val)))\n",
    "print('Train accuracy {} and F1 {}'.format(accuracy_train,f1_train))\n",
    "print('Validation accuracy {} and F1 {}'.format(accuracy_val,f1_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_logistic_CCA_noOH = shap.Explainer(logistic_CCA_noOH,CCA_X_valid)\n",
    "shap_values_logistic_CCA_noOH = explainer_logistic_CCA_noOH(CCA_X_valid)\n",
    "shap.summary_plot(shap_values_logistic_CCA_noOH, CCA_X_valid,plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has lower predictive power than its one-hot encoding correspondance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the EBM. Rather than running SHAP which is not current support, we will use build-in interpret feature to inspect global feature imporant and partial dependent plot instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_CCA_global = ebm_CCA.explain_global(name='EBM')\n",
    "show(ebm_CCA_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the poster, we use SHAP analysis for most important features for each model (except EBM which has its own). All boosted tree model have similar predictive power and most important feature with lightGBM full has highest validation F1. However, we choose to show partial dependent plot and population density from EBM (package interpretML/interpret) since it's easier to understand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading this notebook. Hope this example here would be useful for your research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
